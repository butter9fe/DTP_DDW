{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c48f8bf-05c1-4c52-b185-1a4afe677eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPI: 0.07331603713239443\n",
      "SRFTMP: 0.2071654273582737\n",
      "PM2.5: 0.047208546174921207\n",
      "AQI: 0.09662936333413163\n",
      "CO2: -0.0860017016585033\n",
      "OADR: 0.6020197263555205\n",
      "GDP: 0.06475976806242423\n",
      "HDI: 0.3122010216294061\n",
      "PDBC: -0.18597907140859804\n",
      "OOP: 0.1283595400522436\n",
      "HUM: -0.08793629323573837\n",
      "GFI: 0.10648698388874667\n",
      "TOB: 0.18260354742242713\n",
      "POPDEN: -0.2597068213559275\n",
      "['EPI', 'SRFTMP', 'AQI', 'OADR', 'GDP', 'HDI', 'OOP', 'TOB']: 0.6220435416561766\n"
     ]
    }
   ],
   "source": [
    "from typing import TypeAlias\n",
    "from typing import Optional, Any    \n",
    "\n",
    "Number: TypeAlias = int | float\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as axes\n",
    "import seaborn as sns\n",
    "\n",
    "# Normalize the dataset -> between -1 to 1\n",
    "# yhat = b0 + b1x\n",
    "def normalize_z(array: np.ndarray, columns_means: Optional[np.ndarray]=None, \n",
    "                columns_stds: Optional[np.ndarray]=None) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    assert columns_means is None or columns_means.shape == (1, array.shape[1])\n",
    "    assert columns_stds is None or columns_stds.shape == (1, array.shape[1])\n",
    "    \n",
    "    if columns_means is None: \n",
    "        columns_means = array.mean(axis=0).reshape(1, -1) # reshape output into 1 by N array shape \n",
    "    if columns_stds is None:\n",
    "        columns_stds = array.std(axis=0).reshape(1, -1)\n",
    "\n",
    "    out: np.ndarray = (array - columns_means) / columns_stds\n",
    "    \n",
    "    assert out.shape == array.shape\n",
    "    assert columns_means.shape == (1, array.shape[1])\n",
    "    assert columns_stds.shape == (1, array.shape[1])\n",
    "    return out, columns_means, columns_stds\n",
    "\n",
    "def get_features_targets(df: pd.DataFrame, \n",
    "                         feature_names: list[str], \n",
    "                         target_names: list[str]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df_feature: pd.DataFrame = df[feature_names]\n",
    "    df_target: pd.DataFrame = df[target_names]\n",
    "    return df_feature, df_target\n",
    "\n",
    "def prepare_feature(np_feature: np.ndarray) -> np.ndarray:\n",
    "    # Get the number of rows\n",
    "    m: int = np_feature.shape[0]\n",
    "    \n",
    "    # Create an array of 1s, with shape of m rows and 1 column\n",
    "    ones_array = np.ones((m, 1))\n",
    "    \n",
    "    # Add column of constant 1s in first column\n",
    "    X:np.ndarray = np.concatenate((ones_array, np_feature), axis = 1) # axis = 1 is to concatenate column wise\n",
    "    return X\n",
    "\n",
    "\n",
    "def predict_linreg(array_feature: np.ndarray, beta: np.ndarray, \n",
    "                   means: Optional[np.ndarray]=None, \n",
    "                   stds: Optional[np.ndarray]=None) -> np.ndarray:\n",
    "    assert means is None or means.shape == (1, array_feature.shape[1])\n",
    "    assert stds is None or stds.shape == (1, array_feature.shape[1])\n",
    "    \n",
    "    norm_data, _, _ = normalize_z(array_feature, means, stds) # (1) Standardize the feature using z-normalization\n",
    "    X: np.ndarray = prepare_feature(norm_data) # (2) Change to Numpy array & add column of constant 1s\n",
    "    result = calc_linreg(X, beta) # (3) Predict y values\n",
    "    \n",
    "    assert result.shape == (array_feature.shape[0], 1) # assert that the result vector is m by 1, where m is # data points\n",
    "    return result\n",
    "\n",
    "# yhat = b0hat + b1hat * x\n",
    "def calc_linreg(X: np.ndarray, beta: np.ndarray) -> np.ndarray:\n",
    "    result = np.matmul(X, beta)\n",
    "    \n",
    "    # we need to make sure that the shape of the result array tallies with X \n",
    "    # if we have N data points in the dataset, then the result array should have N x 1 dimension \n",
    "    assert result.shape == (X.shape[0], 1)\n",
    "    return result\n",
    "\n",
    "def split_data(df_feature: pd.DataFrame, df_target: pd.DataFrame, \n",
    "               random_state: Optional[int]=None, \n",
    "               test_size: float=0.5) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    indexes: pd.Index = df_feature.index \n",
    "    if random_state != None: # this is just for the tester in Vocareum, we just need to make sure that the randomness remains \"predictable\" for grading purposes, hence we set the seed \n",
    "        np.random.seed(random_state) \n",
    "    \n",
    "    # find the proportion of test set \n",
    "    k: int = int(test_size * len(indexes))\n",
    "    test_index = np.random.choice(indexes, k, replace=False)\n",
    "    \n",
    "    # find the indexes that are not selected by the test index\n",
    "    train_index = indexes.drop(test_index)\n",
    "    \n",
    "    # time to create the dataframe of feature & target for each set (train & test)\n",
    "    df_feature_train: pd.DataFrame = df_feature.loc[train_index, :]  \n",
    "    df_feature_test: pd.DataFrame  = df_feature.loc[test_index, :]\n",
    "    df_target_train: pd.DataFrame = df_target.loc[train_index, : ]\n",
    "    df_target_test: pd.DataFrame = df_target.loc[test_index, :]\n",
    "    \n",
    "    # this is not a good practice to return elements like this (positional), we can easily swap between them\n",
    "    # a better way is to return it in a data structure\n",
    "    return df_feature_train, df_feature_test, df_target_train, df_target_test\n",
    "\n",
    "def r2_score(y: np.ndarray, ypred: np.ndarray) -> float:\n",
    "    ymean: np.ndarray = np.mean(y)\n",
    "    diff: np.ndarray = y - ymean #(y - ybar)\n",
    "    sstot: np.ndarray = np.matmul(diff.T, diff) # (y - ybar)^2\n",
    "    error: np.ndarray = y - ypred # (y - yhat)\n",
    "    ssres: np.ndarray = np.matmul(error.T, error) # (y - yhat)^2\n",
    "    return 1 - np.squeeze(ssres/sstot) # remember to squeeze the value out of the matrix form because r^2 is a scalar, not a 1-element vector [[r^2]] \n",
    "\n",
    "def mean_squared_error(target: np.ndarray, pred: np.ndarray) -> float:\n",
    "    n: int = target.shape[0] # number of data points \n",
    "    error = target - pred #(y - yhat)^2\n",
    "    error_sq = np.matmul(error.T, error)\n",
    "    return 1/n * np.squeeze(error_sq)\n",
    "\n",
    "# J(B0, B1) = 1/2m * error_sq\n",
    "def compute_cost_linreg(X: np.ndarray, y: np.ndarray, beta: np.ndarray) -> np.ndarray:\n",
    "    m: int = X.shape[0] # get the number of data in the dataset \n",
    "    predicted_y = calc_linreg(X, beta) # Linear Regression value -> yhat\n",
    "    error = predicted_y - y  # this is error is a vector of shape m by 1 -> yhat - y\n",
    "    \n",
    "    # eg: error is [1 2 3] --> we want 1^2 + 2^2 + 3^2\n",
    "    # we can do matmul: [[1 2 3]] ( 1 row 3 columns)  matmul  [1 2 3] (3 rows, 1 col) --> result is 1 row 1 col , e.g: [[14]]\n",
    "    error_sq = np.matmul(error.T, error)\n",
    "    J: np.ndarray = (1/(2*m)) * error_sq\n",
    "    assert J.shape == (1,1) # 1 row 1 column \n",
    "    \n",
    "    # we want to return scalar, so we need to take out the content of J\n",
    "    return np.squeeze(J) # Same as J[0][0] -> returns axes in array with size 1, eg [[1]] -> [1]\n",
    "\n",
    "def gradient_descent_linreg(X: np.ndarray, y: np.ndarray, beta: np.ndarray, \n",
    "                            alpha: float, num_iters: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    # find size of data points\n",
    "    m: int = X.shape[0]\n",
    "    \n",
    "    # create an array to store error value J at each iteration\n",
    "    # it is an num_iters of gd x 1 vector\n",
    "    J_storage: np.ndarray = np.zeros((num_iters, 1))\n",
    "    for n in range(num_iters):\n",
    "        # Eqn: beta1 = beta - (alpha) * (1/m) * (yhat - y) * X\n",
    "        # (1) Compute derivative of error with this current beta\n",
    "        yhat = calc_linreg(X, beta)\n",
    "        # don't forget that matmul here \"loops\" through ALL m datapoints in the train set\n",
    "        deriv: np.ndarray = np.matmul(X.T, (yhat - y)) # (yhat - y) * X\n",
    "        \n",
    "        # (2) Update the beta to be new beta\n",
    "        beta = beta - alpha * (1/m) * deriv \n",
    "        \n",
    "        # (3) Compute error value with this new beta\n",
    "        J_storage[n] = compute_cost_linreg(X, y, beta)\n",
    "\n",
    "    assert beta.shape == (X.shape[1], 1) # beta is a column vector \n",
    "    assert J_storage.shape == (num_iters, 1) \n",
    "    return beta, J_storage\n",
    "\n",
    "# Wraps all the code in CS4 in a function \n",
    "def build_model_linreg(df_feature_train: pd.DataFrame,\n",
    "                       df_target_train: pd.DataFrame,\n",
    "                       beta: Optional[np.ndarray] = None,\n",
    "                       alpha: float = 0.01,\n",
    "                       iterations: int = 1500) -> tuple[dict[str, Any], np.ndarray]:\n",
    "    # Check if initial beta values are given\n",
    "    if beta is None: \n",
    "        beta = np.zeros((df_feature_train.shape[1]+1, 1)) # Add one dimension to the feature_train array because of the b0 coefficient \n",
    "    assert beta.shape == (df_feature_train.shape[1]+1, 1) # To make sure if beta argument is given, then it conforms to the shape of the feature train\n",
    "\n",
    "    # (1): Dataset Preparation\n",
    "    # Normalize the features\n",
    "    array_feature_train_z, means, stds = normalize_z(df_feature_train.to_numpy())\n",
    "\n",
    "    # (2): Use Linear Regression\n",
    "    # Prepare the X matrix and the target vector as ndarray \n",
    "    X: np.ndarray = prepare_feature(array_feature_train_z)\n",
    "    target: np.ndarray = df_target_train.to_numpy()\n",
    "    \n",
    "    # (3) Perform gradient descent\n",
    "    beta, J_storage = gradient_descent_linreg(X, target, beta, alpha, iterations)\n",
    "    \n",
    "    # (4) Store the output in model dictionary \n",
    "    model = {\"beta\": beta, \"means\":means, \"stds\": stds}\n",
    "\n",
    "    # assert the shapes \n",
    "    assert model[\"beta\"].shape == (df_feature_train.shape[1] + 1, 1) # make sure that beta vector is d by 1 \n",
    "    assert model[\"means\"].shape == (1, df_feature_train.shape[1]) # make sure that the means vector is also d-1 by 1 (1 per feature)\n",
    "    assert model[\"stds\"].shape == (1, df_feature_train.shape[1])  # make sure that the stds vector is also d-1 by 1 (1 per feature)\n",
    "    assert J_storage.shape == (iterations, 1) # make sure we have recorded #iterations of error\n",
    "    return model, J_storage\n",
    "\n",
    "def r2_score(y: np.ndarray, ypred: np.ndarray) -> float:\n",
    "    ymean: np.ndarray = np.mean(y)\n",
    "    diff: np.ndarray = y - ymean #(y - ybar)\n",
    "    sstot: np.ndarray = np.matmul(diff.T, diff) # (y - ybar)^2\n",
    "    error: np.ndarray = y - ypred # (y - yhat)\n",
    "    ssres: np.ndarray = np.matmul(error.T, error) # (y - yhat)^2\n",
    "    return 1 - np.squeeze(ssres/sstot) # remember to squeeze the value out of the matrix form because r^2 is a scalar, not a 1-element vector [[r^2]] \n",
    "\n",
    "df: pd.DataFrame = pd.read_csv(\"Test_Asthma_DB.csv\")\n",
    "\n",
    "for index, feature in enumerate([\"EPI\", \"SRFTMP\", \"PM2.5\", \"AQI\", \"CO2\", \"OADR\", \"GDP\", \"HDI\", \"PDBC\", \"OOP\", \"HUM\", \"GFI\", \"TOB\", \"POPDEN\", [\"EPI\", \"SRFTMP\", \"AQI\", \"OADR\", \"GDP\", \"HDI\", \"OOP\", \"TOB\"]]):\n",
    "    # Extract the features and the target\n",
    "    feature_names = feature\n",
    "    if not (isinstance(feature, list)):\n",
    "            feature_names = [feature]\n",
    "            \n",
    "    df_features, df_target = get_features_targets(df, feature_names, [\"LCR_PER_OR\"])\n",
    "\n",
    "    # Split the data set into training and test\n",
    "    df_features_train, df_features_test, df_target_train, df_target_test = split_data(df_features, df_target, random_state=100, test_size=0.3)\n",
    "\n",
    "    # call build_model_linreg() function\n",
    "    model, J_storage = build_model_linreg(df_features_train, df_target_train)\n",
    "\n",
    "    # call the predict_linreg() method\n",
    "    pred = predict_linreg(df_features_test.to_numpy(), model[\"beta\"], model[\"means\"], model[\"stds\"])\n",
    "\n",
    "    # change target test set to a numpy array\n",
    "    target: np.ndarray = df_target_test.to_numpy()\n",
    "\n",
    "    # Calculate r2 score by calling a function\n",
    "    r2: float = r2_score(target, pred)\n",
    "\n",
    "    print(f\"{feature}: {r2}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c54d7-3f8b-4660-a903-6d10ab4aab29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734d6c8-9a1a-40e0-aacb-ec0fed4adcc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
